{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30454aa3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "###### PLEASE DEFINE PATH TO YOUR ROBYN OUTPUT FILE AND MODEL ID\n",
    "\n",
    "# Example usage:\n",
    "source_directory = r\"C:\\Users\\furfirat\\Desktop\\Robyn_202401291649_init\"\n",
    "solID = \"3_346_3\" \n",
    "\n",
    "def copy_json_file(source_dir, json_file_name, new_json_file_name):\n",
    "    source_file_path = os.path.join(source_dir, json_file_name)\n",
    "    destination_file_path = os.path.join(source_dir, new_json_file_name)\n",
    "\n",
    "    try:\n",
    "        if not os.path.isfile(source_file_path):\n",
    "            raise FileNotFoundError(f\"Source JSON file '{json_file_name}' not found in directory '{source_dir}'.\")\n",
    "\n",
    "        shutil.copy(source_file_path, destination_file_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "json_filename = \"RobynModel-models.json\"\n",
    "new_json_file_name = f\"RobynModel-{solID}.json\"\n",
    "copy_json_file(source_directory, json_filename, new_json_file_name)\n",
    "json_file_path = os.path.join(source_directory, new_json_file_name)\n",
    "output_file = os.path.join(source_directory, new_json_file_name)\n",
    "\n",
    "custom_separators = (',', ':')\n",
    "\n",
    "csv_file_path = None\n",
    "for file_name in os.listdir(source_directory):\n",
    "    if file_name.startswith(\"pareto_aggregated\") and file_name.endswith(\".csv\"):\n",
    "        csv_file_path = os.path.join(source_directory, file_name)\n",
    "        break\n",
    "        \n",
    "if csv_file_path:\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "\n",
    "if \"solID\" in df.columns:\n",
    "    df = df[df[\"solID\"].astype(str) == str(solID)]\n",
    "\n",
    "else:\n",
    "    print(\"Column 'solID' not found in the CSV file.\")\n",
    "error_values = df.iloc[0][[\"rsq_train\", \"nrmse_train\", \"nrmse\", \"decomp.rssd\", \"mape\"]].apply(lambda x: round(float(x), 4)).to_dict()\n",
    "\n",
    "df.rename(columns={'rn': 'variable', 'xDecompPerc': 'decompPer', 'xDecompAgg': 'decompAgg', 'cpa_total': 'performance'}, inplace=True)\n",
    "\n",
    "column_order1 = ['variable', 'coef', 'decompPer', 'decompAgg', 'performance', 'mean_response', 'mean_spend']\n",
    "column_order2 = ['variable', 'coef', 'decompPer', 'decompAgg']\n",
    "\n",
    "df = df[column_order1]\n",
    "\n",
    "decimal_cols = ['coef', 'decompPer', 'decompAgg', 'performance', 'mean_response', 'mean_spend']\n",
    "df[decimal_cols] = df[decimal_cols].round(4)\n",
    "\n",
    "variables_to_filter = [\"(Intercept)\", \"trend\", \"season\", \"weekday\", \"holiday\", \"monthly\"]\n",
    "if any(var in df['variable'].values for var in variables_to_filter):\n",
    "    filtered_df1 = df[df['variable'].isin(variables_to_filter)][column_order2]\n",
    "    filtered_df2 = df[~df['variable'].isin(variables_to_filter)]\n",
    "else:\n",
    "    filtered_df1 = df\n",
    "    filtered_df2 = pd.DataFrame()\n",
    "\n",
    "result1 = filtered_df1.to_dict(orient='records')\n",
    "result2 = filtered_df2.to_dict(orient='records')\n",
    "\n",
    "result = result1 + result2\n",
    "\n",
    "for record in result:\n",
    "    if \"performance\" in record and record[\"performance\"] == float(\"inf\"):\n",
    "        record[\"performance\"] = 0\n",
    "\n",
    "exported_model = {\n",
    "    \"ExportedModel\": {\n",
    "        \"select_model\": [solID],\n",
    "        \"ts_validation\": [False],\n",
    "        \"summary\": result,\n",
    "        \"errors\": [error_values]\n",
    "    }\n",
    "}\n",
    "\n",
    "json_file_path = os.path.join(source_directory, new_json_file_name)\n",
    "\n",
    "with open(json_file_path) as file:\n",
    "    existing_data = json.load(file)\n",
    "\n",
    "updated_data = {\n",
    "    \"InputCollect\": existing_data[\"InputCollect\"],\n",
    "    \"ModelsCollect\": existing_data[\"ModelsCollect\"],\n",
    "    \"ExportedModel\": exported_model\n",
    "}\n",
    "\n",
    "\n",
    "with open(output_file, 'w') as file:\n",
    "    json.dump(updated_data, file, indent=3, separators=custom_separators)\n",
    "\n",
    "with open(output_file, 'r') as json_file:\n",
    "    json_str = json_file.read()\n",
    "\n",
    "json_str = json_str.replace(', ', ',')\n",
    "json_str = json_str.replace(',\\n ', ',')\n",
    "\n",
    "with open(output_file, 'w') as json_file:\n",
    "    json_file.write(json_str)\n",
    "    \n",
    "csv_file_path = None\n",
    "for file_name in os.listdir(source_directory):\n",
    "    if file_name.startswith(\"pareto_hyperparameters\") and file_name.endswith(\".csv\"):\n",
    "        csv_file_path = os.path.join(source_directory, file_name)\n",
    "        break\n",
    "        \n",
    "if csv_file_path:\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "\n",
    "with open(json_file_path) as file:\n",
    "    existing_data = json.load(file)\n",
    "\n",
    "adstock_type = existing_data[\"InputCollect\"][\"adstock\"]    \n",
    "\n",
    "filtered_df = df[df[\"solID\"].astype(str) == str(solID)]\n",
    "\n",
    "\n",
    "if \"geometric\" in adstock_type : \n",
    "    columns_to_include = [\"lambda\", \"train_size\"]\n",
    "    columns_to_include += [col for col in filtered_df.columns if col.endswith((\"_alphas\", \"_gammas\", \"_penalty\", \"_thetas\"))]\n",
    "else:\n",
    "    columns_to_include = [\"lambda\", \"train_size\"]\n",
    "    columns_to_include += [col for col in filtered_df.columns if col.endswith((\"_alphas\", \"_gammas\", \"_penalty\", \"_scales\", \"_shapes\"))]\n",
    "\n",
    "hyper_values = {col: [value] for col, value in filtered_df[columns_to_include].iloc[0].items() if pd.notna(value)}\n",
    "\n",
    "hyper_values = dict(sorted(hyper_values.items(), key=lambda x: x[0].lower()))\n",
    "\n",
    "for key, value in hyper_values.items():\n",
    "    hyper_values[key] = [round(value[0], 4)]\n",
    "\n",
    "with open(json_file_path) as file:\n",
    "    existing_data = json.load(file)\n",
    "\n",
    "exported_model = {\n",
    "    \"ExportedModel\": {\n",
    "        \"select_model\": [solID],\n",
    "        \"ts_validation\": [False],\n",
    "        \"summary\": result,\n",
    "        \"errors\": [error_values],\n",
    "        \"hyper_values\": hyper_values\n",
    "    }\n",
    "}\n",
    "\n",
    "updated_data = {\n",
    "    \"InputCollect\": existing_data[\"InputCollect\"],\n",
    "    \"ModelsCollect\": existing_data[\"ModelsCollect\"],\n",
    "    **exported_model  \n",
    "}\n",
    "\n",
    "with open(output_file, 'w') as file:\n",
    "    json.dump(updated_data, file, indent=3, separators=custom_separators)\n",
    "\n",
    "with open(output_file, 'r') as json_file:\n",
    "    json_str = json_file.read()\n",
    "\n",
    "json_str = json_str.replace(', ', ',')\n",
    "\n",
    "with open(output_file, 'w') as json_file:\n",
    "    json_file.write(json_str)\n",
    "\n",
    "with open(json_file_path) as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "hyperparameters = data[\"InputCollect\"][\"hyperparameters\"]\n",
    "\n",
    "hyper_updated = hyperparameters.copy()\n",
    "\n",
    "hyper_updated[\"lambda\"] = [0, 1]\n",
    "\n",
    "csv_file_path = None\n",
    "for file_name in os.listdir(source_directory):\n",
    "    if file_name.startswith(\"pareto_aggregated\") and file_name.endswith(\".csv\"):\n",
    "        csv_file_path = os.path.join(source_directory, file_name)\n",
    "        break\n",
    "        \n",
    "if csv_file_path:\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    train_size = df[\"train_size\"].max()\n",
    "    hyper_updated[\"train_size\"] = [train_size]\n",
    "\n",
    "if \"holiday_penalty\" in data.get(\"ExportedModel\", {}).get(\"hyper_values\", {}):\n",
    "    hyper_updated[\"holiday_penalty\"] = [0, 1]\n",
    "if \"monthly_penalty\" in data.get(\"ExportedModel\", {}).get(\"hyper_values\", {}):\n",
    "    hyper_updated[\"monthly_penalty\"] = [0, 1]  \n",
    "if \"season_penalty\" in data.get(\"ExportedModel\", {}).get(\"hyper_values\", {}):\n",
    "    hyper_updated[\"season_penalty\"] = [0, 1]\n",
    "if \"trend_penalty\" in data.get(\"ExportedModel\", {}).get(\"hyper_values\", {}):\n",
    "    hyper_updated[\"trend_penalty\"] = [0, 1]\n",
    "if \"weekday_penalty\" in data.get(\"ExportedModel\", {}).get(\"hyper_values\", {}):\n",
    "    hyper_updated[\"weekday_penalty\"] = [0, 1]\n",
    "\n",
    "\n",
    "    \n",
    "hyper_values = data.get(\"ExportedModel\", {}).get(\"hyper_values\", {})\n",
    "if \"trend_penalty\" in hyper_values:\n",
    "    hyper_updated[\"trend_penalty\"] = [0, 1]\n",
    "if \"holiday_penalty\" in hyper_values:\n",
    "    hyper_updated[\"holiday_penalty\"] = [0, 1]\n",
    "if \"weekday_penalty\" in hyper_values:\n",
    "    hyper_updated[\"weekday_penalty\"] = [0, 1]\n",
    "if \"monthly_penalty\" in hyper_values:\n",
    "    hyper_updated[\"monthly_penalty\"] = [0, 1]\n",
    "\n",
    "keys_to_reorder = [key for key in hyper_values if key.endswith(\"_penalty\")]\n",
    "\n",
    "variable_order = [entry[\"variable\"] for entry in data[\"ExportedModel\"][\"summary\"]]\n",
    "\n",
    "reordered_keys = sorted(keys_to_reorder, key=lambda key: variable_order.index(key.split(\"_penalty\")[0]))\n",
    "\n",
    "for key in reordered_keys:\n",
    "    hyper_updated[key] = [0, 1]\n",
    "\n",
    "data[\"ExportedModel\"][\"hyper_updated\"] = hyper_updated\n",
    "\n",
    "with open(output_file, 'r') as json_file:\n",
    "    json_str = json_file.read()\n",
    "\n",
    "data[\"ExportedModel\"][\"calibration_constraint\"] = [0.1]\n",
    "data[\"ExportedModel\"][\"cores\"] = [7]\n",
    "\n",
    "csv_file_path = None\n",
    "for file_name in os.listdir(source_directory):\n",
    "    if file_name.startswith(\"pareto_aggregated\") and file_name.endswith(\".csv\"):\n",
    "        csv_file_path = os.path.join(source_directory, file_name)\n",
    "        break\n",
    "\n",
    "if csv_file_path:\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    max_iterations = df[\"iterations\"].max()\n",
    "    max_iterations_rounded = int(max_iterations // 1000) * 1000\n",
    "    max_trials = int(df[\"trial\"].max())\n",
    "\n",
    "    data[\"ExportedModel\"][\"iterations\"] = [max_iterations_rounded]\n",
    "    data[\"ExportedModel\"][\"trials\"] = [max_trials]\n",
    "\n",
    "data[\"ExportedModel\"][\"intercept_sign\"] = [\"non_negative\"]\n",
    "data[\"ExportedModel\"][\"nevergrad_algo\"] = [\"TwoPointsDE\"]\n",
    "\n",
    "has_penalty_key = any(key.endswith(\"_penalty\") for key in data[\"ExportedModel\"][\"hyper_updated\"])\n",
    "\n",
    "add_penalty_factor_dict = {\n",
    "    \"add_penalty_factor\": [True] if has_penalty_key else [False]\n",
    "}\n",
    "\n",
    "data[\"ExportedModel\"][\"seed\"] = [123]\n",
    "data[\"ExportedModel\"][\"pareto_fronts\"] = [3]\n",
    "\n",
    "non_penalty_keys = [key for key in hyper_updated.keys() if not key.endswith(\"_penalty\")]\n",
    "all_same_values = len(set(hyper_updated[key][0] for key in non_penalty_keys)) == 1\n",
    "\n",
    "data[\"ExportedModel\"][\"hyper_fixed\"] = [True] if all_same_values else [False]\n",
    "\n",
    "json_folder_path = json_file_path.replace(\"\\\\\", \"/\")\n",
    "\n",
    "data[\"ExportedModel\"][\"plot_folder\"] = [json_folder_path + \"/\"]\n",
    "\n",
    "with open(json_file_path, 'w') as file:\n",
    "    json.dump(data, file, indent=3, separators=custom_separators)\n",
    "    \n",
    "with open(output_file, 'r') as json_file:\n",
    "    json_str = json_file.read()\n",
    "\n",
    "json_str = re.sub(r'\\[\\s+', '[', json_str)  \n",
    "json_str = re.sub(r'\\s+\\]', ']', json_str) \n",
    "json_str = re.sub(r',[\\s\\n]+(?=[^\\[\\]]*])', ',', json_str)\n",
    "\n",
    "with open(output_file, 'w') as json_file:\n",
    "    json_file.write(json_str)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc",
   "language": "python",
   "name": "pymc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
